「ディープラーニングと物理学　オンライン」とはオンラインWeb会議システムを利用したセミナーです。

登録する際のメールアドレスは、できるだけ大学もしくは研究機関のものをご使用ください。

* ZoomのミーティングURLおよびパスワードは、先着順300名様に限り、登録されたメールアドレスに送信されます。転載・転送は控えてください。
* URLが掲載されたメールは当日の朝までに送られます。

* 参加したい方は下記よりお申し込みください。毎回開催時に参加URLのついたアナウンスのメールを送信します。<br>
[登録フォーム](https://docs.google.com/forms/d/e/1FAIpQLSfiSawgWi9BZF7pFmGsTDD7bOX4kPZKX-BRtPJ4PmwhKe-22A/viewform) <br>
(締切は前日の夜11時までとします)

* 解約フォームは下記でございます。<br>
[解約フォーム](https://docs.google.com/forms/d/e/1FAIpQLSePLXuRLkxVK3CdR_aZBup166mRROk5Z3Ty4zN86_7vSLON2w/viewform)

* 2021年2月以降の講演者の推薦お待ちしています<br>
[講演者の推薦はこちら](https://docs.google.com/forms/d/e/1FAIpQLSfLv6BbJ_s5X-bKiGxaUYWALFixWi23ZRv3EcYFxUgE1R479w/viewform)


* 参加時の表示名は「登録時の名前＠登録した機関名」に設定してください。
* ノイズを防ぐためのミュートへご協力ください。

世話人:　橋本幸士（京都大）、富谷昭夫（理研BNL）、永井佑紀（原子力機構）、田中章詞（理研iTHEMS）（順不同)

本セミナーシリーズは、科学研究費補助金新学術領域研究「[次世代物質探索のための離散幾何学](https://www.math-materials.jp)」の補助を受けております。

* [第30回：　Dimitrios Bachtis 「Quantum field-theoretic machine learning」](#第30回) 7/15
* [第29回：　Lei Wang 「Fermi Flow: Ab-initio study of fermions at finite temperature」](#第29回) 7/1
* [第28回：　蘆田祐人「差分進化を用いた最適なナノ熱機関の探索」](#第28回) 6/17
* [第27回：　今泉允聡「深層学習の汎化誤差解析：損失面由来の暗黙的正則化と深層モデルの二重降下」](#第27回) 6/3
* [第26回：　パネルディスカッション「物理 x 深層学習 の未来」](#第26回) 5/20
* [第25回：　田中章詞「識別器による最適輸送」](#第25回) 5/6
* [第24回：　堀江正信「物理シミュレーションのための同変グラフニューラルネットワーク」](#第24回) 4/22
* [第23回：　富谷昭夫「ゲージ共変なニューラルネットと4次元非可換ゲージ理論への応用」](#第23回) 4/8
* [第22回：　松原祟「エネルギー保存則など望ましい性質を持つ深層学習の設計について」](#第22回) 3/11
* [第21回：　一木輝久「ニューラルネットワークによる結び目の標準化」](#第21回) 2/25
* [第20回：　Hidenori Tanaka & Daniel Kunin「Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics」](#第20回) 2/18
* [第19回：　James Halverson「Neural Networks and Quantum Field Theory」](#第19回) 1/28
* [第18回：　鈴木大慈「無限次元勾配ランジュバン動力学による深層学習の最適化理論と汎化誤差解析」](#第18回) 1/14
* [第17回：　本間希樹「EHTによるブラックホールの撮像とスパースモデリング」](#第17回) 12/10
* [第16回：　入門講義「機械学習と物理」](#第16回) 11/26
* [第15回：　森貴司 「深層学習の汎化の謎をめぐって」](#第15回) 11/12
* [第14回：　野尻美保子 「ミンコフスキー汎関数を用いた機械学習の提案」](#第14回) 10/29
* [第13回：　Kazuhiro Terao 「End-to-End, Machine Learning-based Data Reconstruction for Particle Imaging Neutrino Detectors」](#第13回) 10/15 (ご講演は英語）
* [第12回：　樺島祥介 「スパース線形回帰に対する半解析的ブートストラップ法」](#第12回) 10/1
* [第11回：　藤井啓祐 「NISQ (Noisy Intermediate-Scale Quantum technology) マシンを用いた量子機械学習」](#第11回) 9/17
* [第10回：　斎藤弘樹 「強化学習を用いたボース・アインシュタイン凝縮体の制御」](#第10回) 9/3
* [第9回：　林祐輔「表現学習の熱力学：深層生成モデルの物理法則を求めて」](#第9回) 8/20
* [第8回：　野村悠祐「ボルツマンマシンを用いた量子多体波動関数表現：深層ボルツマンマシンによる厳密な表現と制限ボルツマンマシンによる数値的近似表現」 ](#第8回) 8/6
* [第7回：　本武陽一「物理学者と学習機械の効果的な協業に向けて：学習済み深層ニューラルネットワークからの解釈可能な物理法則抽出」 ](#第7回) 7/30
* [第6回：　吉岡信行「ニューラルネットワークで探る量子多体系の表現」 ](#第6回) 7/9
* [第5回：　福嶋健二「物理学における観測と機械学習：中性子星の事例」 ](#第5回) 6/25
* [第4回：　唐木田亮「深層学習の数理: 統計力学的アプローチ」 ](#第4回) 6/18
* [第3回：　ライトニングトーク](#第3回) 6/11
* [第2回：　橋本幸士「深層学習と時空」](#第2回) 5/28
* [第1回：　永井佑紀「精度保証された機械学習分子動力学法：自己学習ハイブリッドモンテカルロ法」](#第1回) 5/14

# 第30回
日時: 7月15日10:30-11:30(JST)<br>
発表者: Dimitrios Bachtis (Swansea University)<br>
発表題目:Quantum field-theoretic machine learning <br>
概要：
The precise equivalence between discretized Euclidean field theories and a certain class of probabilistic graphical models, namely the mathematical framework of Markov random fields, opens up the opportunity to investigate machine learning from the perspective of quantum field theory. In this talk we will demonstrate, through the Hammersley-Clifford theorem, that the $\phi^{4}$ scalar field theory on a square lattice satisfies the local Markov property and can therefore be recast as a Markov random field. We will then derive from the $\phi^{4}$ theory machine learning algorithms and neural networks which can be viewed as generalizations of conventional neural network architectures. Finally, we will conclude by presenting applications based on the minimization of an asymmetric distance between the probability distribution of the $\phi^{4}$ machine learning algorithms and that of target probability distributions.<br>

# 第29回
日時: 7月1日10:30-11:30(JST)<br>
発表者: Lei Wang (Chinese Academy of Sciences)<br>
[講演スライド](./slides/Coulomb_gas-DLAP.pdf)<br>
発表題目:Fermi Flow: Ab-initio study of fermions at finite temperature <br>
概要：
Fermi Flow is a variational free energy approach to thermal properties of interacting fermions in the continuum. The approach builds on classic works such as Feynman's backflow transformation and DeWitt's quantized point transformation. Crucially, one can leverage modern computing techniques for these physical transformations by exploiting their connection to recent advances in deep learning, such as equivariant normalizing flows and neural ordinary equations. I will discuss promising results of Fermi Flow applied to the uniform electron gas, a fundamental problem in condensed matter and warm dense matter research. <br>

# 第28回
日時: 6月17日10:30-11:30(JST)<br>
発表者: 蘆田祐人（東大）<br>
[講演スライド](./slides/DL_Physics_ashida.pdf)<br>
発表題目:差分進化を用いた最適なナノ熱機関の探索<br>
概要：最適な熱機関を探索する試みはカルノーの古典的な仕事以来、熱統計力学の中心的な課題の一つである。本講演では、進化的計算の一つである差分進化と呼ばれる手法を用いて、相互作用するナノ熱電系のうち熱力学的効率・パワーの意味で「最適」な熱機関を同定する試み[1]について紹介する。（時間が許せば）ベイズ推定の超解像推定への応用に関する研究についても簡単に紹介する。<br>
[1] YA and T. Sagawa, Commun. Phys. 4, 45 (2021).<br>

# 第27回
日時: 6月3日10:30-11:30(JST)<br>
発表者: 今泉允聡（東京大学 先進科学研究機構）<br>
発表題目: 深層学習の汎化誤差解析：損失面由来の暗黙的正則化と深層モデルの二重降下<br>
概要：深層学習が高い汎化性能を達成するが、その原理の理論的解明は未だ発展途上の課題である。本講演では深層学習の性能を記述する、(i)損失面由来の暗黙的正則化、(ii)深層モデルのための二重降下、の2つの理論研究成果を紹介する。<br>
(i)暗黙的正則化は、学習アルゴリズムがニューラルネットワークモデルの自由度を暗黙的に制約することで、深層学習の過適合が防がれていることを主張する。ただし、深層ニューラルネットワークで実現する暗黙的正則化は明らかではなく、有力であるとされていた正則化の仮説（零点や学習初期値近傍）は近年の研究で強い批判を受けている。本研究では、深層ニューラルネットワークの損失面が多くの局所最小値を持ちかつ一定の仮定を満たすとき、この形状が学習アルゴリズム（確率的勾配降下法）の行動を制約し正則化を実現することを理論的に示す。またこのとき、深層ニューラルネットワークが正則化され、この汎化誤差がパラメータ数に依存しない上限を持つことを示す。<br>
(ii)二重降下をはじめとする漸近リスク解析は、モデルから定まる共分散構造のスペクトルを用いて、過剰なパラメータを持つモデルの汎化誤差を解析する理論的枠組みである。近年強い注目を集めて議論が進展しているが、この理論が適用できるのはランダム特徴量モデルやカーネル回帰などの特徴量に対する線形モデルに限られる。よって、層が多いニューラルネットワークなどの深層モデルへの適用可能性は未知数である。本研究は、線形性の制約を置かない一般的なモデル族について、最適化問題が尤度関数で定義されかつ一定の正則性を満たす時、この汎化誤差の上限が漸近リスクの理論に従うことを示す。さらにこの正則性条件を調べることで、並列化ニューラルネットなどの具体的な非線形・深層モデルが二重降下などの理論に従うことを示す。

# 第26回
日時: 5月20日10:30-11:30(JST)<br>
パネルディスカッション: テーマ「物理 x 深層学習 の未来」<br>
パネラー: 大槻東巳（上智大理工）、樺島祥介(東大理／知の物理学研究センター)、田中章詞（理研iTHEMS/AIP 慶応数理）、富谷昭夫（理研BNL）、永井佑紀(原子力機構)<br>
情報提供: 野尻美保子(高エネルギー加速器研究機構)、福嶋健二 (東大理)<br>
概要: 2017年に開始された"Deep learning and physics"会合は、現在およそ1000名の登録者の方々が集うコミュニティに成長しました。そこで、これまでの「物理 x 機械学習」の研究の進展を振り返り、これからの研究の進展の可能性と方向を議論する機会を設けます。パネラーの方々そして情報提供の方々から今後の可能性についてプレゼンをいただき、それを元にパネラー間そして聴衆の方々と議論をすることで、これからの研究を加速する種とし、本コミュニティでの問題共有を行うとともに、チャンレンジする大きな課題を明確化します。

# 第25回
日時: 5月6日10:30-11:30(JST)<br>
発表者: 田中章詞（理研iTHEMS/AIP 慶応数理）<br>
[講演スライド](./slides/20210506_atanaka.pdf)<br>
発表題目:識別器による最適輸送<br>
概要:深層学習を用いた潜在変数モデルの代表的な二つの例として、変分自己符号化器と敵対的生成ネットワークが知られています。これらは共に二つのネットワークを訓練するモデルです。変分自己符号化器は画像などのデータを潜在変数にエンコードする符号化器と、それをデコードして画像に戻す復号化器から構成されます。敵対的生成ネットワークは潜在変数から新たな画像を作り出す生成器と、それを本物と見分ける識別器から構成されます。変分自己符号化器ではそれぞれのネットワークが訓練後も明確な応用（符号化、復号化）を持ちますが、敵対的生成ネットワークの識別器は一見明確な応用先を持たないように思えます。本講演では最適輸送理論のアイデアを借用することで識別器を用いて生成器の画像生成の品質改善が可能[1]なことと、その他の識別器の応用について説明します。<br>
[1] Tanaka, A.. “Discriminator optimal transport.” NeurIPS (2019)<br>

# 第24回
日時: 4月22日10:30-11:30(JST)<br>
発表者: 堀江正信（株式会社科学計算総合研究所、筑波大学システム情報工学研究群）<br>
[講演スライド](./slides/isogcn.pdf)<br>
発表題目:物理シミュレーションのための同変グラフニューラルネットワーク
<br> 
概要：グラフニューラルネットワーク (GNN) は、物理シミュレーションで広く用いられるメッシュデータに対して効率的に学習を行える機械学習モデルであり、これを用いることにより物理シミュレーションを高速化・高効率化することが期待されている。しかし、一般的な GNN は推論に時間がかかること、回転や平行移動といった座標変換に対する対称性 (同変性) を考慮していないなどの問題があった。そこで講演者らは、高速に推論できかつ座標変換に対する同変性を持つ GNN を提案した [1]。提案手法では、既存の同変 GNN では不可能であった、100 万頂点ほどの大規模グラフに対する高速な推論が可能であることが示されている。本講演では、提案手法およびその関連研究について述べる。なお、提案手法に関するソースコードは https://github.com/yellowshippo/isogcn-iclr2021 からダウンロード可能である。<br>
[1] M. Horie, N. Morita, T. Hishinuma, Y. Ihara, N. Mitsume. Isometric Transformation Invariant and Equivariant Graph Convolutional Networks, In International Conference on Learning Representations (ICLR), 2021 (arXiv: 2005.06316).<br>


# 第23回
日時: 4月8日10:30-11:30(JST)<br>
発表者: 富谷昭夫（理研BNL）<br>
[講演スライド](./slides/cov-nn-dlap.pdf)<br>
発表題目:ゲージ共変なニューラルネットと4次元非可換ゲージ理論への応用（Gauge covariant neural network for 4 dimensional non-abelian gauge theory）
<br> 
概要：ニューラルネットの文脈でも物理においても対称性は本質的に重要である。本講演では、講演者らが開発したゲージ対称性を保つニューラルネットワークについて議論する。このニューラルネットワークは、リー群に値を取るベクトル場の間の写像を対称性を保ちながら実現する。従来のゲージ場のスメアリングやgradient flowは、固定パラメータを持つランク2テンソルのニューラルネットワークやneural ODEとみなすことができることを発見した。さらに活性化関数(スメアリングにおける規格化)の局所性を利用して、ゲージ共変ニューラルネットワークのバックプロパゲーションを導き出した。さらに学習則がうまくいくことを確認するために、動的フェルミオンを含む非可換ゲージ理論の系に対して、共変ニューラルネットワーク近似作用を用いた自己学習HMC（SLHMC）をもちいてシミュレーションを行った。結果としてHMCの結果をSLHMCが再現することを確認した。本講演は、[1]に基づく。<br>
[1] Akio Tomiya, Yuki Nagai, https://arxiv.org/abs/2103.11965<br>

# 第22回
日時: 3月11日10:30-11:30(JST)<br>
発表者: 松原祟（大阪大学大学院基礎工学研究科）<br>
[講演スライドweb版](./slides/Matsubara_web.pdf)<br>
発表題目: エネルギー保存則など望ましい性質を持つ深層学習の設計について<br>
概要: "深い"ニューラルネットワークである深層学習は高い柔軟性を持ち，大規模なデータを学習することで，自動的に高度な意思決定システムを構築できるのだと説明されている．しかし，実際に深層学習が注目される契機となったのは畳み込みニューラルネットワークであり，平行移動不変性という特殊な性質を持っている．この性質によって，対象の位置のずれや微小な変形に対して堅牢な識別が可能になっている．類似したアイデアは，データの集合を入力として受け取るDeep Setsや，ノード間の順番を考慮しないグラフ畳み込みにも見られ，それぞれの分野で高い性能を発揮している．成功した深層学習は，ノーフリーランチ定理そのままに，何らかの性質を持つよう設計されている．<br>
このような考え方を物理学の世界に広げると，望ましい性質として物理の法則が挙げられる．近年ではハミルトニアンニューラルネットワークといい，ハミルトン系の基礎方程式を模倣することで，連続時間でエネルギー保存則が成り立つ物理シミュレーションを可能にするフレームワークが提案されている[1]．講演者らはさらに，ハミルトン系に限定されずエネルギーで記述された物理現象について，エネルギー保存則・散逸則を 離散時間において 厳密に保証するフレームワークを提案した[2]．本講演では，これらの研究に関する近年の動向を紹介するとともに，深層学習から見た物理学への期待についても述べる．<br>
[1] Sam Greydanus, Misko Dzamba, and Jason Yosinski, "Hamiltonian Neural Networks, " Advances in Neural Information Processing Systems (NeurIPS), 2019.<br>
[2] Takashi Matsubara, Ai Ishikawa, and Takaharu Yaguchi, "Deep Energy-Based Modeling of Discrete-Time Physics," Advances in Neural Information Processing Systems (NeurIPS), 2020.<br>

# 第21回
日時: 2月25日10:30-11:30(JST)<br>
発表者: 一木輝久（名古屋大学未来社会創造機構）<br>
発表題目: ニューラルネットワークによる結び目の標準化<br>
概要: 典型的なニューラルネットワークはaffine変換と活性化関数を繰り返すことで実装されている．したがって活性化関数が狭義単調な連続関数の場合、ニューラルネットワークは入力データ空間の位相的性質を保存する変換を表現していることになる[1, 2]．そこで本講演ではニューラルネットワークを、トポロジーを不変に保つ変形を行う変換器として利用することを提唱する．具体的には結び目の上に配置した荷電粒子の反発エネルギーを最小化することによって、グチャグチャに折りたたまれた「汚い」結び目を、よりエネルギーの低い「標準的な」結び目へ変形するようニューラルネットワークの学習を行う．荷電粒子の物理モデルに勾配法などを適用すればニューラルネットワークは必要ないように思われるが、純粋な物理モデルで長さ一定の紐の上に束縛された粒子の運動を数値的に取り扱うのは容易ではない．このような利点を踏まえ、ニューラルネットワークは結び目を分類できるだろうか？という問いに対する取り組みを紹介する．<br>
[1] C. Olah, “Neural Networks, manifolds, and topology”, Blog post (2014).<br>
[2] M. Hajij and K. Istvan, “A topological framework for deep learning”, arXiv:2008.13697.<br>


# 第20回
日時: 2月18日10:30-11:30(JST)<br>
発表者: Hidenori Tanaka (Group Head & Senior Scientist at NTT Physics & Informatics Laboratories,Visiting Scholar at Stanford University)<br>
Daniel Kunin (Stanford University)<br>
[Slide](./slides/Neural_Mechanics_slides.pdf)<br>
発表題目: Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics<br>
（ご講演は英語）<br>
概要: Predicting the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of a network in high-dimensional parameter space undergoes discrete finite steps along complex stochastic gradients derived from real-world datasets. We circumvent this obstacle through a unifying theoretical framework based on intrinsic symmetries embedded in a network's architecture that are present for any dataset. We show that any such symmetry imposes stringent geometric constraints on gradients and Hessians, leading to an associated conservation law in the continuous-time limit of stochastic gradient descent (SGD), akin to Noether's theorem in physics. We further show that finite learning rates used in practice can actually break these symmetry induced conservation laws. We apply tools from finite difference methods to derive modified gradient flow, a differential equation that better approximates the numerical trajectory taken by SGD at finite learning rates. We combine modified gradient flow with our framework of symmetries to derive exact integral expressions for the dynamics of certain parameter combinations. We empirically validate our analytic predictions for learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset.<br>
Refs:<br>
[1] "Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics" (ICLR 2021)
Daniel Kunin*, Javier Sagastuy-Brena, Surya Ganguli, Daniel L.K. Yamins, Hidenori Tanaka*<br>
[2] "Pruning neural networks without any data by iteratively conserving synaptic flow" (NeurIPS 2020)
Hidenori Tanaka*, Daniel Kunin*, Daniel L. K. Yamins, Surya Ganguli


# 第19回
日時: 1月28日10:30-11:30(JST)<br>
発表者: James Halverson (Northeastern University, NSF AI Institute for Artificial Intelligence and Fundamental Interactions, co-organizes of Physics ∩ ML)<br>
[講演スライド](./slides/NN-QFT_DLAP.pdf)<br>
発表題目: Neural Networks and Quantum Field Theory<br>
（ご講演は英語）<br>
概要: We propose a theoretical understanding of neural networks in terms of Wilsonian effective field theory. The correspondence relies on the fact that many asymptotic neural networks are drawn from Gaussian processes, the analog of non-interacting field theories. Moving away from the asymptotic limit yields a non-Gaussian process and corresponds to turning on particle interactions, allowing for the computation of correlation functions of neural network outputs with Feynman diagrams. Minimal non-Gaussian process likelihoods are determined by the most relevant non-Gaussian terms, according to the flow in their coefficients induced by the Wilsonian renormalization group. This yields a direct connection between overparameterization and simplicity of neural network likelihoods. Whether the coefficients are constants or functions may be understood in terms of GP limit symmetries, as expected from 't Hooft's technical naturalness. General theoretical calculations are matched to neural network experiments in the simplest class of models allowing the correspondence. Our formalism is valid for any of the many architectures that becomes a GP in an asymptotic limit, a property preserved under certain types of training.<br>
[1] arXiv:2008.08601

# 第18回
日時: 1月14日10:30-11:30(JST)<br>
発表者: 鈴木大慈(東京大学大学院 情報理工学系研究科数理情報学専攻)<br>
[講演スライド](./slides/Physics2021_public.pdf)<br>
発表題目: 無限次元勾配ランジュバン動力学による深層学習の最適化理論と汎化誤差解析<br>
概要: これまで統計的学習理論の枠組みにおいて深層学習の優位性が様々な
研究によって解明されてきたが，それらの多くはモデルの非凸性に起因
している．その意味で，深層学習の統計的な「良さ」を引き出すためには
，非凸最適化問題を避けることは難しいと考えられる．本講演では，
無限次元勾配ランジュバン動力学を用いて深層学習の非凸最適化問題を
解く手法を考察し，その汎化誤差を評価するための枠組みを提案する．
既存の平均場理論やニューラルタンジェントカーネル (NTK) といった
典型的な枠組みでは，大域的最適性を示すために横幅をサンプルサイズ
に合わせて無限大まで漸近させる必要があった．本講演では，より自然な
解析を実現するために深層学習の最適化を無限次元ランジュバン動力学
の観点から解析し，その大域的最適性および汎化性能の解析を行う．
本枠組みを用いることで，横幅が有限の場合も無限の場合も統一的に扱え，
さらに解がベイズ推定量に対応することから汎化誤差のバウンド及び
余剰誤差の速い収束レートが示せる．また，本手法によって得られた
推定量の予測誤差が任意の線形推定量を優越するような例が作れること
も示し，深層学習の有用性を最適化の保証付きで示す．

# 第17回
日時: 12月10日10:30-11:30(JST)<br>
発表者: 本間希樹（国立天文台 水沢VLBI観測所）<br>
発表題目: EHTによるブラックホールの撮像とスパースモデリング<br>
概要：2019年4月、Event Horizon Telescope（EHT）プロジェクトによって、楕円銀河M87の中心にある巨大ブラックホールの影の写真が公表され、巨大ブラックホールの存在を示す視覚的な証拠として大きな話題となった。この写真は、世界中のミリ波電波望遠鏡を結ぶVLBI（Very Long Baseline Interferometry：超長基線電波干渉計）の観測手法を用いて撮影されたものである。電波干渉計の画像処理の基礎方程式は２次元フーリエ変換であるが、望遠鏡の台数が限られることから基礎方程式は劣決定となり、直接に逆フーリエ変換を実行することができない。このような問題を効率的に解くための手法として、我々はスパース性を制約として適切な解を得る「スパースモデリング」を用いて画像処理を行う解析手法を開発してきた。今回の講演では、電波干渉計によるブラックホール観測においてスパースモデリングや機械学習的なアプローチがどのように活用されているかについて紹介し、今後の展望についても合わせて述べたい。

# 第16回
日時: 11月26日10:30-12:00(JST)<br>
入門講義「機械学習と物理」<br>

第1部<br>
発表者: 田中章詞 (RIKEN iTHEMS/AIP)<br>
発表題目: 深層学習入門<br>
[講演スライド](./slides/DLAP2020intro_tanaka.pdf)<br>
概要:入門者向けにpythonの文法から深層学習フレームワークのミニマムな使い方の講義を行います。講義はgoogle colaboratoryを使って、実際にコードを触りながら行う予定です。<br>

第2部<br>
発表者: 富谷昭夫 (RIKEN BNL)<br>
発表題目: ニューラルネットを使った2次元イジング模型の相検出<br>
[手順書](./slides/Tomiya_howto_dlap2020.pdf)<br>
[Jupyter notebook(github)](./slides/DLAP2020_ising_detection.ipynb)<br>
[講演スライド](./slides/DLAP2020_lecture_tomiya_pub.pdf)<br>
概要: 本講義では、tensorflow/kerasを用いてNature Physics掲載の論文[1] の結果であるニューラルネットを使った2次元イジング模型の相転移の検出をおこないます。統計力学の初歩程度の知識を仮定します。<br>
[1] Juan Carrasquilla & Roger G. Melko, Nature Physics volume 13, pages431–434(2017)
<br>

# 第15回
日時: 11月12日10:30-11:30(JST)<br>
発表者: 森貴司(RIKEN CEMS)<br>
[講演スライド](./slides/DL_Physics2020_mori.pdf)<br>
発表題目: 深層学習の汎化の謎をめぐって<br>
概要：現代的な深層学習の応用は，モデルに含まれるパラメータの数が訓練データのサンプル数よりも大きい，いわゆる過剰パラメータ領域 (overparameterized regime) でなされている．そのような領域で深刻な過剰適合 (overfitting) を示すことなく高い汎化性能が得られることは深層学習の大きな謎の一つである．この謎を理論的に解明しようとするときに重要だと思われる要素として，(i)モデルの特徴（ネットワークのタイプや深さ，広さなど），(ii)データの持つ構造（データの有効次元，局所性，対称性など），(iii)最適化アルゴリズムの特性（確率的勾配法などのダイナミクス）が挙げられる．<br>

(i)として最も単純な全結合ニューラルネットを考えると，モデルを特徴づけるのは隠れ層の数（深さ）および各隠れ層のニューロン数（広さ）である．深さと広さのどちらかを増やすことによって過剰パラメータ領域に到達できるが，これまでの研究では主に広さを増やしていったときの影響が調べられてきた．特に広さ無限大の極限でNeural Tangent Kernel (NTK)の理論が得られ，大きな注目の的になっている．一方で，ネットワークを深くしていったときに汎化性能がどのように変化するかについては理論的に不明な点が多い．<br>

私たちはネットワークの深さおよび広さが汎化にどのように影響するかを(ii)データの構造，特に分類ラベルの局所性の観点から調べた[1]．その結果，ネットワークの深さが汎化に良い影響をもたらすかどうかは分類ラベルの局所性に依存することがわかった．さらに，ネットワークの広さを増やしていっても必ずしもNTKの結果に収束しないことを示唆する結果が得られた．本講演では深層学習の汎化の謎とその理論的解明に向けた最近の研究についてレビューし，上記の私たちの結果について説明する．時間がゆるせば(iii)の話題として，汎化性能を向上させるための確率的勾配法の改良についての私たちの最近の研究[2]についても簡単に紹介したい．<br>

[1] Takashi Mori and Masahito Ueda, arXiv:2005.12488<br>
[2] Takashi Mori and Masahito Ueda, arXiv:2009.13094<br>

# 第14回
日時: 10月29日10:30-11:30(JST)<br>
発表者: 野尻　美保子(高エネルギー加速器研究機構)<br>
[講演スライド](./slides/nojiri.pdf)<br>
発表題目: ミンコフスキー汎関数を用いた機械学習の提案<br>
概要：LHCなどで行われているコライダー実験では深層学習の利用がさかんです。この中で、強い相互作用（QCD) によって作られるジェット（ハドロン束）がどのようなプロセスから作られたかを深層学習で分類する問題は、新現象の探索に成果をあげています。深層学習を使ったジェット画像分類は、QCDとしてよい IRC(Infrared collinear) safe な指標だけを使ったジェット分類とくらべて、パフォーマンスが良いことがしられていました。<br>
今回のトークでは、ジェットの画像をミンコフスキー汎関数で一旦「測度空間」にマップした量(MS=Minkowski Sequence ) をIRC safe な量に加えた深層学習が、CNN による分類を超える結果をだすことを複数のベンチマークで示します。典型的なジェットの画像診断は、1000程度のinput を使いますが、新しい方法では、100 程度の情報しか使わないため、高速で安定した解析が可能です。MS は、CNN のフィルタリングアウトプットとして表現することができるため、CNNの画像診断には含まれていると考えるのが自然です。CNN のアウトプットにMS を追加することで、CNN がMS 以外の情報に集中させることが可能です。<br>
深層学習を利用すると物理解析のパフォーマンスが向上する場合には、通常の解析では取り入れられていない相関が含まれていると考えるべきでしょう。我々の研究の場合は、QCD の非摂動的な量をミンコフスキー汎関数を用いて少数の量で表すことで、摂動的な量と非摂動的な量との相関を安定的に利用できるようにしたと解釈することができます。この研究では深層学習のパフォーマンスに満足するのではなく、よりよい特徴空間を発見することが重要であるということをあらためて感じました。今後の イベント生成パッケージの向上、新物理の発見等への応用を検討していますが、宇宙などでの深層学習や、一般的な画像解析への応用についてもご意見をいただければと思っています。<br>
[1] M.Nojiri et al., to appear.<br>
[2] Neural Network-based Top Tagger with Two-Point Energy Correlations and Geometry of Soft Emissions. Amit Chakraborty, Sung Hak Lim, Mihoko M. Nojiri, Michihisa Takeuchi. JHEP 07 (2020) 111, JHEP 20 (2020) 111 (https://arxiv.org/abs/2003.11787)<br>
[3] 物理の人にはこれもよいと思います。K. R. Mecke, in Statistical Physics and Spatial Statistics, edited by K. R. Mecke and D. Stoyan (Springer Berlin Heidelberg, Berlin, Heidelberg, 2000) pp. 111–184.<br>

# 第13回
日時: 10月15日10:30-11:30(JST)<br>
発表者: Kazuhiro Terao (SLAC スタンフォード国立加速器研究所)<br>
[講演スライド](./slides/2020-10-14-JapanSeminar.pdf)<br>
（ご講演は英語）
発表題目: End-to-End, Machine Learning-based Data Reconstruction for Particle Imaging Neutrino Detectors<br>
概要：With firm evidence of neutrino oscillation and measurements of mixing parameters, neutrino experiments are entering the high precision measurement era. The detector is becoming larger and denser to gain high statistics of measurements, and detector technologies evolve toward particle imaging, essentially a hi-resolution "camera", in order to capture every single detail of particles produced in a neutrino interaction. The forefront of such detector technologies is a Liquid Argon Time Projection Chamber (LArTPC), which is capable of recording images of charged particle tracks with breathtaking resolution. Such detailed information will allow LArTPCs to perform accurate particle identification and calorimetry, making it the detector of choice for many current and future neutrino experiments. However, analyzing hi-resolution imaging data can be challenging, requiring the development of many algorithms to identify and assemble features of the events in order to reconstruct neutrino interactions. In the recent years, we have been investigating a new approach using deep neural networks (DNNs), a modern solution to a pattern recognition for image-like data in the field of Computer Vision. A modern DNN can be applied for various types of problems such as data reconstruction tasks including interaction vertex identification, pixel clustering, particle type and flow reconstruction. In this talk I will discuss the challenges of data reconstruction for imaging detectors, recent work and future plans for developing a full LArTPC data reconstruction chain using DNNs.


# 第12回
日時: 10月1日10:30-11:30(JST)<br>
発表者: 樺島祥介(東大理／知の物理学研究センター)<br>
[講演スライド](./slides/SemianalyticBS.pdf)<br>
発表題目:スパース線形回帰に対する半解析的ブートストラップ法<br>
概要：データからのパラメータ推定では必ず統計的なゆらぎが生じる．
古典的な推測統計学では，母集団分布と推定量の分布を「解析的に」関係づける
ことで，得られた推定量の確からしさを定量的に評価する．これにより，与えられた
信頼度の下での幅をもたせた推定（＝区間推定）や一定の有意水準の下での仮説の妥当性評価
（＝統計的検定）が可能になる．残念ながら，こうした解析的アプローチが可能なのは，
正規分布からサンプリングされたデータからの平均値の推定など，単純な
状況の場合に事実上限られる．母集団を表現する分布がわからない場合，あるいは，
わかっていても推定量が多次元の場合や導出に複雑な手続きを要する場合には
推定量がしたがう分布を解析的に導くことは難しい．
とはいえ，推定結果の不確実性を評価することはどのような場合であっても重要である．
ブートストラップ法はこうした困難に対する有力な対処法である．
この方法では，手元にあるデータが表す経験分布を母集団とみなす``plug-in principle''にしたがい，
経験分布からの復元抽出によって得られるデータに対する推定を何度も繰り返す
こと（＝リサンプリング）で推定量の分布を「数値的に」評価する．
ブートストラップ法はほぼすべての統計的推定／機械学習の方法に適用できる一方で，
リサンプリングに要する計算量がボトルネックになっている．
我々は，ランダム系の統計力学で発展した解析法であるレプリカ法と平均場近似を
組み合わせることで，スパース推定の代表的な方法であるleast absolute shrinkage and
selection operator (LASSO)に対し，リサンプリングを行うことなく，推定量の分布を
評価する近似法を開発した[1,2]．その内容について紹介する．<br>
[1] T. Obuchi and Y. Kabashima, Semi-Analytic Resampling in Lasso,
Journal of Machine Learning Research 20 (70), 1-33 (2019)<br>
[2] T. Takahashi and Y. Kabashima, Semi-analytic approximate stability selection
for correlated data in generalized linear models, arXiv:2003.08670;
to appear in Journal of Statistical Mechanics: Theory and Experiment<br>

# 第11回
日時: 9月17日10:30-11:30(JST)<br>
発表者: 藤井啓祐(阪大基礎工)<br>
発表題目:NISQ (Noisy Intermediate-Scale Quantum technology) マシンを用いた量子機械学習<br>
[講演スライド](./slides/DeepLPhys_Fujii.pdf)<br>
講演概要: 
近年、GoogleやIBMなどの巨大IT企業が量子コンピュータの開発競争を繰り広げている。昨年Googleは、50量子ビットを超えるサイズの量子コンピュータを実現し、特定のタスクにおいてスーパーコンピュータよりも圧倒的に速く計算ができることを実証した。しかしながら、素因数分解など高速性が証明されている複雑な量子アルゴリズムの実行には、量子コンピュータの規模がまだ小さくノイズレベルも高い。このような、ノイズを含む小・中規模の量子コンピュータは、NISQ（Noisy Intermediate-Scale Quantum technology）と呼ばれている。このNISQマシンを、機械学習、量子化学計算、最適化などに応用するための研究が現在盛んに進められている。本セミナーでは、このような量子コンピュータを取り囲む現状を紹介し、NISQマシンを機械学習へと応用する、量子機械学習について紹介する。特に、量子ダイナミクスを用いて時系列データ処理を行う、量子レザバー計算[1]や、量子回路をモデルとして教師あり学習を行う量子回路学習[2]、そして、量子状態に特徴量を埋め込み量子コンピュータを用いてカーネルを推定する量子カーネル推定法[3]について、我々の研究も交えてご紹介する。<br>

[1] K. Fujii and K. Nakajima “Harnessing Disordered-Ensemble Quantum Dynamics for Machine Learning”, Phys. Rev. Applied 8, 24030 (2017).<br>
[2] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii, “Quantum Circuit Learning”, Phys. Rev. A 98, 032309 (2018). <br>
[3] T. Kusumoto, K. Mitarai, K. Fujii, and M. Kitagawa, and M. Negoro, “Experimental quantum kernel machine learning with nuclear spins in a solid”, arXiv:1911.12021.<br>

# 第10回
日時: 9月3日10:30-11:30(JST)<br>
発表者: 斎藤弘樹(電気通信大学基盤理工学専攻)<br>
発表題目:強化学習を用いたボース・アインシュタイン凝縮体の制御<br>
[講演スライド](./slides/DLAP2020Saito.pdf)<br>
講演概要: 
強化学習とは、「エージェント」がある「環境」の中で行動し「報酬」を受け取るという過程を経ながら、最適な行動を選択できるように学習するという、機械
学習の一手法である。最も有名な応用例は、囲碁で人間を打ち負かしたAlphaGoだろう。そこでは Deep-Q learning と呼ばれる、強化学習に深層ニューラルネットワークを組み入れた手法が使われている。最近、Deep-Q learning を物理の問題に応用するという研究も盛んに行われている。本講演では、超低温原子気体のボース・アインシュタイン凝縮体(BEC)のダイナミクスの制御に Deep-Q learning を応用した例を紹介する[1]。外部ポテンシャルによってBECをかき回すと、量子渦が生成されることが知られているが、Deep-Q learning によって外部ポテンシャルの動きを学習させ、望みの状態の量子渦が生成できることを示す。<br>
[1] H. Saito, J. Phys. Soc. Jpn. 89, 074006 (2020)<br>


# 第9回
日時: 8月20日10:30-11:30(JST)<br>
講演者: 林祐輔（Japan Digital Design, Inc. ただし講演は個人としての活動）<br>
講演題目: 表現学習の熱力学：深層生成モデルの物理法則を求めて<br>
[講演スライド](./slides/DLAP2020slideyh.pdf)<br>
講演概要: 本講演では，深層生成モデルを熱力学と結びつけるいくつかの研究について紹介する．ベイズ統計と熱力学には形式的アナロジーが成り立つことが知られている[1]．その核にあるのは，ベイズ統計における損失関数が，熱力学における自由エネルギーと対応するというアイディアである．近年ではこのアナロジーを深層生成モデルにあてはめて，訓練が終了して平衡状態に落ち着いた深層生成モデルに対して熱力学第一法則や第二法則に相当するものが成り立つことを調べた研究がある[2]．ここでも損失関数として自由エネルギーが登場し，完全な熱力学関数のように振る舞う．しかし，このままでは損失関数と自由エネルギーの対応はアナロジーの域を出ず，統計的学習の背後にあるはずの学習プロセスを拘束する諸条件，“物理法則”に相当するものがみえてこないと講演者は感じている．深層生成モデルについて熱力学第一法則や第二法則以上の拘束条件は見出せないのだろうか？最近になって深層生成モデルを使った転移学習のプロセスに，準静的過程のアナロジーを持ち込んだ研究があらわれた[3]．深層生成モデルの損失関数は再構成誤差項や正則化項，分類誤差項などの要素からなるが，これらを温度にみたてて“等温準静的過程”を考えるのである．このような“熱力学的操作”を考えることで，損失関数の特定の要素の値を一定に保つ転移学習を行うことができる．この熱力学的操作を組み合わせればサイクルを考えることができ，その最大熱効率を与えるサイクルがカルノーサイクルであることを示すことができる．これは非平衡過程を含む転移学習プロセスの一つの拘束条件になっている．<br>
[1] Sumio Watanabe, Mathematical Theory of Bayesian Statistics, CRC Press (2018)<br>
[2] Alemi and Fischer, TherML: Thermodynamics of Machine Learning, arXiv:1807.04162 (2018)<br>
[3] Gao and Chaudhari, A Free-Energy Principle for Representation Learning, ICLR Workshop Integration of Deep Neural Models and Differential Equations (2020)<br>
[4] Hideaki Shimazaki, Neural Engine Hypothesis (Dynamic Neuroscience pp267-291), Springer (2017)<br>

# 第8回
日時:  8月6日10:30-11:30(JST)<br>
発表者: 野村悠祐（理研CEMS）<br>
発表題目: ボルツマンマシンを用いた量子多体波動関数表現：深層ボルツマンマシンによる厳密な表現と制限ボルツマンマシンによる数値的近似表現<br>
[講演スライド](./slides/DLAP_2020_Nomura.pdf)<br>
概要： 指数関数的に大きな次元を持つ量子多体系の波動関数を有限個のパラメータで精度よく表すことは、物性物理のみならず素粒子、原子核、量子化学などに共通するグランドチャレンジである。本講演では機械学習で用いられるボルツマンマシンが量子多体波動関数表現に有用であることを紹介する。まず隠れ層が二層ある深層ボルツマンマシン(DBM)を用いて基底状態の波動関数を任意の精度で解析的に表現することが可能であることを示す[1]。これは経路積分を包含するより一般的な量子古典対応のフレームワークを提供するが、物理量計算に隠れ層自由度をモンテカルロサンプリングする必要性があるため負符号問題が生じる場合がある[1]。一方で、隠れ層が一層に制限された制限ボルツマンマシン(RBM)では、DBMのように解析的な表現はできないものの、数値的にパラメータを最適化することで、量子多体波動関数の表現が可能になる。RBMの良いところは解析的に隠れ層自由度をトレースアウトできるところであり、量子系の数値計算を行う上では負符号問題を回避できる点でDBMより利点がある場合がある。RBMを用いた手法は、フェルミオン系、フェルミオン-ボソン結合系、フラストレーションのあるスピン系な度への適用が進んでおり、それら一連の研究について紹介する[2-4]。<br>
[1] G. Carleo, Y. Nomura, and M. Imada, Nat. Commun. 9, 5322 (2018)<br>
[2] Y. Nomura, A. S. Darmawan, Y. Yamaji, and M. Imada, Phys. Rev. B 96, 205152 (2017)<br>
[3] Y. Nomura, J. Phys. Soc. Jpn. 89, 054706 (2020)<br>
[4] Y. Nomura and M. Imada, arXiv:2005.14142<br>

# 第7回
日時:  7月30日10:30-11:30(JST)<br>
発表者: 本武陽一 (統計数理研究所)<br>
[講演スライド](./slides/DLAP2020_7_mototake.pdf)<br>
発表題目: 物理学者と学習機械の効果的な協業に向けて：学習済み深層ニューラルネットワークからの解釈可能な物理法則抽出<br>
概要： 本講演では、力学系時系列データを学習した深層ニューラルネットワーク（DNN）から、解釈可能な物理則を抽出する手法を提案する[1]。
物理学は科学者の物理的洞察力によって大きく発展してきたが、アクティブマタや磁性体の磁区構造といった複雑な秩序構造を持つ系で洞察力を働かせることは時に困難である。これに対して、深層学習を始めとした複雑な高次元データをモデル化できる機械学習を用いた物理モデル構築手法の開発が近年活発に研究されている[1]。我々は、複雑なデータの内挿的モデル構築を得意とする機械学習と、物理的洞察によって大胆な理論の外挿を実現できる科学者の協業を実現することが重要と考え、これに繋がり得る手法を開発した。具体的には、物理データを学習したDNNから解釈可能な物理情報を抽出し科学者に提供することを目標として、有限自由度古典ハミルトン力学系とみなせる時系列データを学習したDNNから、系の隠れた保存則を推論する新しい手法を開発した。手法は、Noetherの定理と効果的なサンプリング法を元にDNNから力学系の対称性を抽出することで保存則を推論する。手法でDNNに課される前提は、学習に成功したDNNで広く成り立つと考えられている多様体仮説[3,4]のみであるため、手法は広範なDNNモデルに適用可能と考えられる。講演では、安定状態にある生物の集団運動を模擬した時系列データに提案手法を適用した事例等を紹介する。

[1] Interpretable Conservation Law Estimation by Deriving the Symmetries of Dynamics from Trained Deep Neural Networks, Y. Mototake, arXiv:2001.00111.<br>
[2] Hamiltonian neural networks, G. Samuel, M. Dzamba, and J. Yosinski, NeurIPS 2019 (arXiv:1906.01563).<br>
[3] Representation Learning: A Review and New Perspectives, Y. Bengio, C. Aaron, and V. Pascal, IEEE Trans. Pattern Anal. Mach. Intell., 35.8 (2013): 1798-1828.<br>
[4] Efficient representation of low-dimensional manifolds using deep networks, R. Basri and D.W., Jacobs, ICLR 2017 (arXiv:1602.04723).<br>


# 第6回
日時:  7月9日10:30-11:30(JST)<br>
発表者: 吉岡信行 (理研)<br>
発表題目: ニューラルネットワークで探る量子多体系の表現<br>
[講演スライド](./slides/202007_yoshioka_DLAP_seminar_send.pdf)<br>
従来より高い表現能力を持つことが知られていたニューラルネットワークは、近年の計算資源の向上・最適化手法の発達により、多くの機械学習タスクにおいて成功を収めている。
さらに、画像・音声などといった古典データに限らず、量子多体系の基底状態・励起状態などの表現に対しても有効であることが示された [1]。
次元性に依拠しない変分関数の構造は、量子多体系の大規模計算において用いられてきたテンソルネットワークと相補的に活用されることで、
多体現象の研究のフロンティアを押し拡げる役割を担うことが期待されている。
本講演では、ボルツマンマシンと呼ばれる種類のニューラルネットワークを導入し、その性質・応用について概観したのち、
講演者らによって見出された、開放量子多体系への適用可能性について議論する [2, 3]。

[1] G. Carleo and M. Troyer, Science 355, 602 (2017).<br>
[2] N. Yoshioka and R. Hamazaki, Phys. Rev. B 99, 214306 (2019).<br>
[3] N. Yoshioka et al., in preparation.<br>

# 第5回
日時: 6月25日10:30-11:30(JST)<br>
発表者: 福嶋健二 (東京大学)<br>
発表題目: 物理学における観測と機械学習：中性子星の事例<br>
[講演スライド](./slides/fukushima.pdf)<br>
概要: 学習、特に教師あり学習や強化学習とは、一言で表せば、最適化問題を解くということである。物理学は古来、最適化問題の効率向上に心血を注いできた学問であり、機械学習との親和性は高い。物理学における実験データの回帰分析に機械学習を応用するのは自然な発想だろう。例えば高エネルギー物理学ではジェットの識別に機械学習がその威力を発揮していることはよく知られている。しかし物理学の多くの回帰分析では、誤差のついたデータから誤差をつけたデータを引き出す必要があり、学習モデルはインプットが真値からずれている、ということも適切に学習せねばならない。本講演では中性子星の観測データから状態方程式を構築する問題を具体的な事例として採り上げ[1,2]、ひとつのアプローチ法を提案する。最後に、観測誤差を取り入れた学習が、ニューラルネットワークの重みの初期値依存性問題と密接に関係していることを議論する。<br>
[1] Y. Fujimoto, K. Fukushima, K. Murase, Phys.Rev.D 98 (2018) 2, 023019<br>
[2] Y. Fujimoto, K. Fukushima, K. Murase, Phys.Rev.D 101 (2020) 5, 054016<br>

# 第4回
日時: 2020年6月18日10:30-11:30(JST)<br>
発表者: 唐木田 亮 (産総研)<br>
発表題目: 深層学習の数理: 統計力学的アプローチ<br>

[登録フォーム](https://docs.google.com/forms/d/e/1FAIpQLSdJtfmkcJJ8gagNZ0_XJH7m5l1gPA4v0VsfxJUri5uxAOGSNA/viewform)

[講演スライド](./slides/DL_and_phys_2020_karakida_20200618.pdf)

概要: 本講演では, 深層学習の数理的な理解を目的とした近年の統計力学的解析を紹介する. 具体的には, 深層学習の平均場理論, ランダム行列理論, さらにNeural Tangent Kernel (NTK)理論をとおして学習ダイナミクスの理解へとつながる一連の流れを解説する. 
深層学習では, 性能の理論的な保証や, 最適なモデルや学習手法の設定に多くの課題があり, 数理的な研究が日進月歩で進んでいる. その中で統計力学的なアプローチは, 深層学習におけるパラメータ初期値がランダム行列であること, モデルの大自由度極限(ニューラルネットワーク各層の"幅"が十分に大きいこと)に着目した枠組みで, 様々なモデルアーキテクチャに普遍的に適応できる点に特徴がある. 平均場理論は, 勾配の大きさを巨視的変数として定式化することで, 訓練が進みやすいモデルとパラメータの設定を相転移によって定量的に説明する[1]. NTK理論は, 初期値まわりの摂動の範囲で学習が進む設定があることを発見し, 学習の大域収束と汎化性能に一定の理解を与える[2]. 
本講演ではこのような統計力学的アプローチを, 歴史的な背景も含めつつ簡単に概観したい. また, このアプローチを使うことで, 再急降下法のハイパーパラメータ(学習率)の設定指針[3]やBatch NormalizationとLayer Normalizationの効果の違い[4]のようなアルゴリズムの問題に対して, 定量的な説明を与えられることも紹介する.

[1] Deep information propagation. Samuel S Schoenholz, Justin Gilmer, Surya Ganguli and Jascha Sohl-Dickstein, ICLR 2017 (arXiv:1611.01232). <br>
[2] Neural tangent kernel: Convergence and generalization in neural networks. Arthur Jacot, Franck Gabriel and Clément Hongler, NeurIPS 2018 (arXiv:1806.07572). <br>
[3] Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach. RK, Shotaro Akaho and Shun-ichi Amari, AISTATS 2019 (arXiv:1806.01316).<br>
[4] The Normalization Method for Alleviating Pathological Sharpness in Wide Neural Networks. RK, Shotaro Akaho and Shun-ichi Amari, NeurIPS 2019 (arXiv:1906.02926).<br>


# 第3回　

第三回(6/11)は、ライトニングトークで構成されます。<br>
日時: 2020年6月11日10:30-11:30(JST)<br>
発表者：希望者<br>

講演者: 野村悠祐 (理研CEMS)<br>
講演題目: 深層ボルツマンマシンを用いた量子多体波動関数の厳密な構築<br>
講演概要: 量子多体系の波動関数を深層ボルツマンマシンを用いて厳密に表現する方法を紹介する(Carleo, Nomura, Imada, Nature Communications 2018)。<br>

[講演スライド](./slides/Nomura_presentation_2020_06_11.pdf)

講演者: 林祐輔 (Japan Digital Design, Inc. ただし講演は個人としての活動)<br>
講演題目: 表現学習の確率熱力学的解釈<br>
講演概要: 表現学習とは，画像，音声，自然言語，時系列データといった多様な生データから，目的のタスクを解くために意味のある特徴量を生成する方法を，統計モデル自身に考えさせるアプローチのことを指す．この発表では，表現学習と確率熱力学の間にアナロジーが成り立つことを紹介し，準静的過程に対応する熱力学的操作が転移学習のようなデータのドメインを跨ぐ学習において有用であることを説明する．<br>

[講演スライド](./slides/Hayashi_DLAP2020.pdf)

講演者: 北沢正清 (大阪大学)<br>
講演題目: SU(3)非可換ゲージ理論のトポロジー分類への機械学習の適用<br>
講演概要: 4次元畳み込みニューラルネットワークを用いて、SU(3)非可換ゲージ理論の多次元データを学習し、トポロジカル電荷を推定した。99%という高い正答率を得た成果などを報告する。<br>

[講演スライド](./slides/Kitazawa_200611DLAP_NNQ.pdf)


講演者: 大塚啓(KEK)<br>
講演題目: 深層学習を用いた弦理論のランドスケープ<br>
講演概要: 本講演では、背景磁場のあるCalabi-Yau 多様体上にコンパクト化されたヘテロ型弦理論に対して、深層学習の一種であるオートエンコーダーによる次元削減とクラスタリングを適用する。 特に、フェルミオンの世代数と余剰次元空間の曲率に強い相関があることを紹介する[1]。[1] H. Otsuka and K. Takemoto, JHEP 05 (2020), 047.<br>

[講演スライド](./slides/Otsuka_20200611.pdf)

講演後には、参加者に「さらに詳しく聞きたい講演」についてのアンケートを実施し、そのアンケート結果をもとに、今後のオンラインセミナーの講演を決めていこうと考えています。<br>

[発表希望者用フォーム](https://docs.google.com/forms/d/e/1FAIpQLSfFsyop94o5mV_2ab6qB_PjpWGUN_77C_dZ44hjmXGC_MHEKg/viewform)

[参加希望者（発表なし）用フォーム](https://docs.google.com/forms/d/e/1FAIpQLSfXiD5kB106iFpglgvTa_TjvoDH03EPFRB1hkSS8N0GZNyrjQ/viewform)


ライトニングトークの構成は以下の通りです：<br>
　１）ご講演は３-10分間、その後の質疑応答は２分間、合計で５-15分間となります。<br>
　　（講演時間が終了すると座長から終了のお知らせがありますのでご協力お願いします）<br>
　２）スライドはpdf (pptもしくはkeynote)数枚です。<br>
　　　講演タイトルと所属、お名前、メールなどの連絡先を記載ください。<br>
　　　見本として[次のもの](./slidesample.pdf)をご参考に。<br>
　３）講演＋質疑時間の５分間が終了した後は、チャット機能で、参加者からの質問にお答えください。<br>
　４）ご講演タイトル（仮）を、上記登録フォームで記入ください。<br>
発表者希望者の締切は6/9の夜11時とします。<br>
発表内容は、ご自身あるいは共同研究者のオリジナルな研究に限ります。<br>

# 第2回
日時: 2020年5月28日10:30-11:30(JST)<br>
発表者: 橋本幸士(大阪大学)<br>
発表題目: 深層学習と時空

[登録フォーム](https://docs.google.com/forms/d/e/1FAIpQLSeWYyvnJC-sHLrit6d-RebOaaDeBFEFc3wsMtlASIJRniLGEg/viewform)

[講演スライド](./slides/Hashimoto20200528.pdf)

概要：
本講演では、深層ニューラルネットワークを時空とみなせるかどうかについて、議論する。特に、量子重力理論として研究が進むホログラフィー原理そのものを深層学習とみなす手法について紹介する。
学習されたニューラルネットワークの解釈可能性は、機械学習を物理学に応用する際に最も重要な観点の一つである。ニューラルネットワークの構造を対称性などにより規定することで制約を加え解釈可能にすることは学習を効率的にすることにつながっている。一方で、深層ボルツマンマシンはそもそもスピン系が原子配列しているものの一般化と考えられることから、ニューラルネットワークそのものが時空として機能している例が存在している。このような中で、量子重力理論と機械学習の融合を目指す研究が生まれた。この20年間の量子重力理論の研究において、最も重要な位置を占めているのはホログラフィー原理[1]であり、そこでは、量子重力理論と等価な、重力を含まない（時空次元の低い）量子系が存在し、重力の時空はそこから創発する。このホログラフィー原理で創発する時空を、教師つき学習の深層ニューラルネットワークと同一視し、時空のメトリックを重みと考えることで、創発時空を量子系のデータから決定する手法[2][3][4]を本講演では紹介する。

[1] The Large N limit of superconformal field theories and supergravity Juan Martin Maldacena Published in: Int.J.Theor.Phys. 38 (1999) 1113-1133, Adv.Theor.Math.Phys. 2 (1998) 231-252 • e-Print: hep-th/9711200 [hep-th]<br>
[2]Deep learning and the AdS/CFT correspondence Koji Hashimoto, Sotaro Sugishita, Akinori Tanaka, Akio Tomiya Published in: Phys.Rev.D 98 (2018) 4, 046019 • e-Print: 1802.08313 [hep-th]<br>
[3] Deep Learning and Holographic QCD Koji Hashimoto, Sotaro Sugishita, Akinori Tanaka, Akio Tomiya
Published in: Phys.Rev.D 98 (2018) 10, 106014 • e-Print: 1809.10536 [hep-th]<br>
[4] Deep Learning and AdS/QCD Tetsuya Akutagawa, Koji Hashimoto, Takayuki Sumimoto e-Print: 2005.02636 [hep-th]<br>


# 第1回
日時: 2020年5月14日10:30-11:30(JST)<br>
発表者: 永井佑紀(原子力機構)<br>
発表題目: 精度保証された機械学習分子動力学法：自己学習ハイブリッドモンテカルロ法 

[登録フォーム](https://docs.google.com/forms/d/e/1FAIpQLSedDoIH3RW6skBAzu84BkZ8yJTapYxC237BztfnPGhVdGgLLg/viewform)

[講演スライド](./slides/Nagai_SLHMC20200514.pdf)

概要：
本講演では、2008年に開発された機械学習分子動力学法について解説するとともに、学習精度に計算精度が依存しない機械学習シミュレーションである自己学習ハイブリッドモンテカルロ法について紹介する。<br>
第一原理計算で得られたポテンシャルを再現するようなニューラルネットワーク(ANN)を構築して分子動力学を実行するのが機械学習分子動力学法である。ANNを構築する際の最適なトレーニングデータは、元々の第一原理分子動力学法で生成される原子配置とそのポテンシャルである。しかしながら、第一原理分子動力学法の計算負荷が高いために機械学習分子動力学法を用いるのであるから、元々の第一原理分子動力学法でデータを集めるのは本末転倒である。そのため、通常は、様々な原子配置とそのポテンシャルデータを大量に作成することで、目的の機械学習分子動力学法と同じようなポテンシャルを生成するANNを構築している。しかしながら、構築されたANNが元々の第一原理計算のポテンシャルを再現するという保証はない。さらに、4元素以上で構成されるような系の場合には、長時間の機械学習分子動力学法では計算が不安定になることがあり、機械学習分子動力学法の計算の精度や妥当性については常に慎重な議論が必要であった。<br>
本講演では、自己学習モンテカルロ法のアイディア[1]を用いることで、得られた結果が統計的に厳密にオリジナルの第一原理計算分子動力学法の計算結果と等しい手法を開発したことを報告する[2]。
そして、その手法を用いて、4元素系でも精度よく計算ができることを示すため、フォノン誘起超伝導体YNi2B2Cのフォノン計算の結果を報告する。<br>

[1] J. Liu, Y. Qi, Z. Y. Meng, and L. Fu, Phys. Rev. B 95, 041101(R) (2017).; J. Liu, H. Shen, Y. Qi, Z. Y. Meng, and L. Fu, Phys. Rev. B 95, 241104(R) (2017).; YN, H, Shen, Y. Qi, J. Liu, and L. Fu, Phys. Rev. B 96, 161102(R) (2017)<br>
[2] YN, M. Okumura, K. Kobayashi and M. Shiga, arXiv:1909.02255<br>

# 過去のワークショップ研究会
[Deep Learning and physics2019](http://kabuto.phys.sci.osaka-u.ac.jp/~koji/workshop/DLAP2019/)<br>
[Deep Learning and physics2018](http://kabuto.phys.sci.osaka-u.ac.jp/~koji/workshop/DLAP2018/)<br>
[Deep Learning and physics](http://kabuto.phys.sci.osaka-u.ac.jp/~koji/workshop/tsrp/Deep_Lerning.html)

